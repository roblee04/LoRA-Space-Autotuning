import yaml

# Your YAML string
yaml_string = """
base_model: TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T
model_type: LlamaForCausalLM
tokenizer_type: LlamaTokenizer
is_llama_derived_model: true

load_in_8bit: false
load_in_4bit: true
strict: false

datasets:
  - path: mhenrichsen/alpaca_2k_test
      type: alpaca
      dataset_prepared_path:
      val_set_size: 0.05
      output_dir: ./qlora-out

      adapter: qlora
      lora_model_dir:

      sequence_len: 1096
      sample_packing: true
      pad_to_sequence_len: true

      LORA_PARAMETERS - lora_r: 32
      LORA_PARAMETERS - lora_alpha: 16
      LORA_PARAMETERS - lora_dropout: 0.05
      LORA_PARAMETERS - lora_target_modules:
      LORA_PARAMETERS - lora_target_linear: true
      LORA_PARAMETERS - lora_fan_in_fan_out:

      wandb_project:
      wandb_entity:
      wandb_watch:
      wandb_name:
      wandb_log_model:

      mlflow_experiment_name: colab-example

      OPTIMIZER_PARAMETER - gradient_accumulation_steps: 1
      OPTIMIZER_PARAMETER - micro_batch_size: 1
      OPTIMIZER_PARAMETER - num_epochs: 4
      OPTIMIZER_PARAMETER - max_steps: 20
      OPTIMIZER_PARAMETER - optimizer: paged_adamw_32bit
      OPTIMIZER_PARAMETER - lr_scheduler: cosine
      OPTIMIZER_PARAMETER - learning_rate: 0.0002

      train_on_inputs: false
      group_by_length: false
      bf16: false
      fp16: true
      tf32: false

      gradient_checkpointing: true
      early_stopping_patience:
      resume_from_checkpoint:
      local_rank:
      logging_steps: 1
      xformers_attention:
      flash_attention: false

      warmup_steps: 10
      evals_per_epoch:
      saves_per_epoch:
      debug:
      deepspeed:
      weight_decay: 0.0
      fsdp:
      fsdp_config:
      special_tokens:

      """

# Convert the YAML string to a Python dictionary
yaml_dict = yaml.safe_load(yaml_string)

# Specify your file path
file_path = 'test_axolotl.yaml'

# Write the YAML file
with open(file_path, 'w') as file:
    yaml.dump(yaml_dict, file)

